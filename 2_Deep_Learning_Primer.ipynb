{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1ml2Iv4Ecff"
   },
   "source": [
    "# **Deep Learning Primer**\n",
    "## Outline\n",
    "- Fundamental concepts of deep learning.\n",
    "- Mathematical model of a neuron.\n",
    "- Overview of neural network structures and common architectures.\n",
    "- Optimizers, gradient descent, and backpropagation algorithms.\n",
    "- Introduction to TensorFlow and PyTorch for deep learning applications.\n",
    "- **Hands-on Lab:** Image classification using TensorFlow or PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb71pDyOEcfh"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNmHgd-ZEcfm"
   },
   "source": [
    "## **The Evolution of Artificial Intelligence**\n",
    "\n",
    "#### The Birth of Artificial Intelligence\n",
    "\n",
    "- Artificial Intelligence (AI) emerged as a field in the mid-20th century.\n",
    "- Symbolic reasoning was the dominant approach initially.\n",
    "\n",
    "#### Early Successes: **Expert Systems**\n",
    "\n",
    "- Symbolic reasoning led to early successes, including the development of expert systems.\n",
    "- Expert systems were computer programs capable of **mimicking human expertise** in specific problem domains.\n",
    "\n",
    "#### The Challenge of Scaling Knowledge\n",
    "\n",
    "- Symbolic reasoning faced limitations in scaling knowledge.\n",
    "- Extracting, representing, and maintaining knowledge in computer-based systems proved complex and costly.\n",
    "\n",
    "#### The AI Winter of the 1970s\n",
    "\n",
    "- These challenges culminated in what became known as the **\"AI Winter\"** during the 1970s.\n",
    "- The AI Winter represented a period of reduced optimism and funding for AI research.\n",
    "- The **scalability** and **practicality** of symbolic reasoning-based approaches came into question.\n",
    "\n",
    "<img src=\"./images/history-of-ai.png\"  align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e2vlQ60Ecfm"
   },
   "source": [
    "## **Evolution of Artificial Intelligence Approaches**\n",
    "\n",
    "- Over time, AI approaches have evolved due to **cheaper computing** resources and increased **data availability**.\n",
    "- Neural network approaches have gained prominence\n",
    "  - Often outperforming humans in areas like **computer vision** and **speech understanding**\n",
    "\n",
    "#### The Transformation of Chess Programs\n",
    "\n",
    "- **Early Chess Programs:**\n",
    "  - Early chess programs relied on **search algorithms** to evaluate possible moves.\n",
    "  - The **alpha-beta pruning** search algorithm was a significant development\n",
    "  - These programs performed well in the endgame but **struggled** at the beginning due to vast search spaces.\n",
    "\n",
    "- **Case-Based Reasoning:**\n",
    "  - To improve early-game performance, case-based reasoning was introduced.\n",
    "  - Programs looked for cases in the knowledge base similar to the current game position.\n",
    "\n",
    "- **Modern Chess Programs:**\n",
    "  - Today's chess programs excel due to **neural networks** and **reinforcement learning**.\n",
    "  - They learn by playing against themselves, adapting and improving rapidly.\n",
    "\n",
    "#### Evolution of \"Talking Programs\"\n",
    "\n",
    "- **Early \"Talking Programs\" (e.g., Eliza):**\n",
    "  - Early conversational programs used simple grammatical rules and sentence re-formulation.\n",
    "  \n",
    "- **Modern Virtual Assistants (Cortana, Siri, Google Assistant):**\n",
    "  - Modern virtual assistants employ hybrid systems.\n",
    "  - They use neural networks for **speech-to-text** conversion and **intent recognition**.\n",
    "  - Reasoning and explicit algorithms are applied to execute actions.\n",
    "\n",
    "- **The Future of AI Dialogue Systems:**\n",
    "  - Future developments may lead to entirely neural-based models handling dialogue.\n",
    "  - Models like GPT and Turing-NLG demonstrate significant progress in natural language understanding and generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHo8LY4lEcfn"
   },
   "source": [
    "## **The Rise of Neural Networks**\n",
    "\n",
    "#### Emergence of Large Public Datasets\n",
    "\n",
    "- The significant growth in neural network research began around **2010**.\n",
    "- The availability of large public datasets played a crucial role in this development\n",
    "- **ImageNet**, a collection of around **14 million annotated images**, led to the ImageNet Large Scale Visual Recognition Challenge.\n",
    "\n",
    "#### Convolutional Neural Networks (CNNs) Revolutionize Image Classification\n",
    "\n",
    "- In 2012, **Convolutional Neural Networks (CNNs)** were first applied to image classification.\n",
    "- This breakthrough led to a substantial reduction in classification errors, from nearly **30%** to **16.4%**\n",
    "\n",
    "#### Achieving Human-Level Accuracy\n",
    "\n",
    "- In 2015, the **ResNet** architecture from Microsoft Research achieved human-level accuracy in image classification.\n",
    "- This marked a significant milestone in neural network research.\n",
    "\n",
    "#### Neural Network Success Stories\n",
    "\n",
    "- Over the years, neural networks have demonstrated remarkable success in various tasks:\n",
    "\n",
    "| Year | Task Achieved Human Parity |\n",
    "|------|---------------------------|\n",
    "| 2015 | Image Classification       |\n",
    "| 2016 | Conversational Speech Recognition |\n",
    "| 2018 | Automatic Machine Translation (Chinese-to-English) |\n",
    "| 2020 | Image Captioning           |\n",
    "\n",
    "#### The Era of Large Language Models\n",
    "\n",
    "- Recent years have witnessed tremendous success with large language models like **BERT** and **GPT-3**.\n",
    "- The availability of **vast amounts of general text data** has enabled training models to understand text structure and meaning.\n",
    "- These models are **pre-trained** on extensive text collections and then fine-tuned for specific tasks.\n",
    "- Natural Language Processing (NLP) has benefited immensely from these advancements.\n",
    "\n",
    "<img src=\"./images/ilsvrc.gif\" width=800 align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMsmwAaWEcfn"
   },
   "source": [
    "## Discussion\n",
    "\n",
    "- Identify where AI is most effectively utilized.\n",
    "- AI applications are prevalent across various domains, enhancing user experiences and enabling new functionalities.\n",
    "  - Mapping Applications\n",
    "  - Speech-to-Text Services\n",
    "  - Video Games\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a0cTW1hEcfn"
   },
   "source": [
    "## **Mathematical Models of Intelligence: Neural Networks**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wsko/hands-on-gen-ai-2/main/images/neural.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- Since the mid-20th century, researchers have experimented with mathematical models for intelligence.\n",
    "  - In recent years, one approach has seen remarkable success: **neural networks**.\n",
    "\n",
    "- Neural networks, often referred to as **Artificial Neural Networks (ANNs)**\n",
    "  - Mathematical models inspired by the structure and function of the human brain\n",
    "- ANNs serve as models, not actual networks of biological neurons.\n",
    "\n",
    "<img src=\"./images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9QBB6EPEcfo"
   },
   "source": [
    "# Biological Basis of Neural Cells\n",
    "\n",
    "<img src=\"./images/neuron1.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "- In biology, the brain is composed of neural cells\n",
    "- Each neural cell has multiple \"inputs\" (axons) and an output (dendrite).\n",
    "- Axons and dendrites are capable of conducting electrical signals.\n",
    "- The connections between axons and dendrites can vary in conductivity, regulated by neuromediators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2Vbg7uhEcfo"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s1X_rceEcfo"
   },
   "source": [
    "## **Mathematical Abstraction of a Neuron**\n",
    "\n",
    "- The simplest mathematical model of a neuron includes multiple inputs X_1, ..., X_N and an output Y, along with a series of weights W_1, ..., W_N.\n",
    "- The output is calculated using the formula:\n",
    "\n",
    "    <img src=\"./images/netout.png\" align=\"center\"/>\n",
    "\n",
    "\n",
    "    - Here, f represents a non-linear activation function.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "#### Historical Perspective\n",
    "\n",
    "- Early models of neurons were introduced in the classic paper titled \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" by Warren McCullock and Walter Pitts in 1943.\n",
    "- Donald Hebb further contributed to the field with his book \"The Organization of Behavior: A Neuropsychological Theory,\" where he proposed methods for training such neural networks.\n",
    "\n",
    "\n",
    "<img src=\"./images/neuron.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-B43oXjEcfp"
   },
   "source": [
    "<img src=\"./images/rumerlhart_hinton-01.png\" width=800 height=auto align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh1D9C5QEcfp"
   },
   "source": [
    "- In 1986, Rumelhard, Hinton, and Williams presented a technique known as backpropagation in a Nature Letter.\n",
    "  - Backpropagation was shown to be useful for training multi-layer neural networks.\n",
    "- Although the idea of backpropagation was not exclusive to or created by Rumelhard, Hinton, and Williams, their publication in Nature Letter prompted a new era of research into neural networks.\n",
    "\n",
    "\n",
    "\n",
    "Neural Networks:\n",
    "  - Propagate signals forward from the input to the output layers\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wsko/hands-on-gen-ai-2/main/images/forward.png\" align=\"center\" width = \"600\"/>\n",
    "\n",
    "  - Propagate the error backwards from the output back into the network\n",
    "    - Backpropagation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wsko/hands-on-gen-ai-2/main/images/backpropage.png\" align=\"center\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxzjtvfVEcfp"
   },
   "source": [
    "- Although Hinton made significant contributions to the development of modern deep learning in the 1980s, it would take some time for the advancements we see today to materialize.\n",
    "- We can begin experimenting with some deep learning concepts now. However, to conduct proper experimentation, we require data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAcJOUiJEcfq"
   },
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KphQ0TMOEcfq"
   },
   "source": [
    "## **Machine Learning vs Deep Learning (Deep Neural Network)**\n",
    "\n",
    "- Terminology:\n",
    "  - \"Traditional\" (not deep) machine learning uses structured data and uses a distinct algorithm such as logistic regression or random forest or SVM etc.\n",
    "  - Deep learning is for unstrctured data. The algorithm is a network of simple learners (neurons which are similar to logistic regression) arranged into multiple deep layers\n",
    "- Traditional machine learning approaches require feature representations that are designed by humans\n",
    "- In deep learning, the focus is on **optimizing the weights of the model** to make the most accurate prediction without requiring explicit feature engineering\n",
    "\n",
    "\n",
    "<img src=\"./images/dl1.png\" width=500 height=auto align=\"center\" />\n",
    "\n",
    "\n",
    "- Deep learning learns data representation first.\n",
    "  - One of the key strengths of deep learning is its ability to effectively **learn complex patterns in data**\n",
    "\n",
    "<img src=\"./images/dl2.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n",
    "\n",
    "- Deep learning applies a multi-layer process for learning rich hierarchical  features (i.e., data representations)\n",
    "  - Input image pixels → Edges → Textures → Parts → Objects\n",
    "\n",
    "<img src=\"./images/dl3.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Slide credit: Param Vir Singh – Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbYwjveFEcfq"
   },
   "source": [
    "## **Why DL is useful?**\n",
    "\n",
    "- Deep learning offers a versatile and adaptable framework for representing various types of data, including visual, text, and linguistic information.\n",
    "- It can learn in both **supervised** (?) and **unsupervised** (?) manners, and is recognized as an effective end-to-end learning system\n",
    "  - However, deep learning requires a significant amount of training data to achieve optimal results.\n",
    "- Since around 2010, deep learning has consistently outperformed other machine learning techniques, initially in areas such as vision and speech, and later in natural language processing and other applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf5JDVSTEcfr"
   },
   "source": [
    "- NNs use nonlinear mapping of the inputs x to the outputs f(x) to compute complex decision boundaries\n",
    "- But then, why use deeper NNs?\n",
    "  - The fact that deep NNs work better is an empirical observation\n",
    "  - Mathematically, deep NNs have the same representational power as a one-layer NN\n",
    "\n",
    "[<img src=\"./images/dl4.png\" width=400 height=auto align=\"center\"/>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nPFjiNMEcfs"
   },
   "source": [
    "## **Neural Network Example**\n",
    "\n",
    "- The task is to recognize digits that are handwritten, using the MNIST dataset.\n",
    "- Each pixel's intensity is taken as an input feature, and the goal is to determine the digit class as the output.\n",
    "\n",
    "<img src=\"./images/dl5.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<img src=\"./images/dl6.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPAFD-FCEcfs"
   },
   "source": [
    "## **Neural Networks**\n",
    "\n",
    "- Neural networks are composed of hidden layers that contain neurons, which are computational units.\n",
    "- A single neuron is responsible for mapping a set of inputs to a numerical output\n",
    "  - Denoted as 𝑓:𝑅^𝐾→𝑅, where 𝑅^𝐾 represents a K-dimensional input space and 𝑅 represents the output space.\n",
    "\n",
    "<img src=\"./images/dl7.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n",
    "\n",
    "- A NN with one hidden layer and one output layer\n",
    "\n",
    "<img src=\"./images/dl8.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqPsz9diEcft"
   },
   "source": [
    "# Tensorflow Playground\n",
    "\n",
    "TensorFlow Playground is an interactive web-based tool that allows you to explore and experiment with neural networks. It provides a visual interface where you can adjust various parameters and observe how they affect the network's behavior.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Architecture Design:** You can design and customize the architecture of your neural network by adding or removing layers, adjusting the number of neurons, and selecting different activation functions.\n",
    "\n",
    "2. **Data Generation:** TensorFlow Playground provides various predefined datasets and patterns that you can use to train your network. You can also create custom datasets by drawing points directly on the graph.\n",
    "\n",
    "3. **Training and Testing:** The tool enables you to train your network using different optimization algorithms and loss functions. You can adjust the learning rate, batch size, and regularization parameters. Additionally, you can split the dataset into training and testing sets to evaluate the network's performance.\n",
    "\n",
    "4. **Visualizations:** TensorFlow Playground offers real-time visualizations of the network's training progress, such as loss curves, decision boundaries, and neuron activations. These visualizations help you understand how the network learns and how it makes predictions.\n",
    "\n",
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### How to Use TensorFlow Playground\n",
    "\n",
    "1. Open the TensorFlow Playground website in your web browser.\n",
    "\n",
    "2. Select a dataset from the left sidebar or create your own by drawing points on the graph.\n",
    "\n",
    "3. Customize the architecture of the neural network by adjusting the parameters in the right sidebar.\n",
    "\n",
    "4. Configure the training settings, such as the optimization algorithm, learning rate, and batch size.\n",
    "\n",
    "5. Start the training process by clicking the \"Play\" button.\n",
    "\n",
    "6. Observe the visualizations and monitor the training progress on the right side of the screen.\n",
    "\n",
    "7. Experiment with different settings and architectures to see how they affect the network's performance.\n",
    "\n",
    "8. Once you're satisfied with the results, you can export the trained model for further use in TensorFlow or other frameworks.\n",
    "\n",
    "\n",
    "\n",
    "[link](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.90335&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "<img src=\"./images/dl10.png\" width=800 height=auto align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySmRB4IpEcft"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aST57ASEcft"
   },
   "source": [
    "## **Deep Neural Network**\n",
    "\n",
    "- Deep neural networks (DNNs) are characterized by having a large number of hidden layers.\n",
    "- These hidden layers typically consist of fully-connected (also known as dense) layers, which are sometimes referred to as Multi-Layer Perceptrons (MLPs).\n",
    "- In such layers, each neuron is connected to every neuron in the next layer.\n",
    "\n",
    "<img src=\"./images/dl11.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fajKtvRlEcfu"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJk8XA2GEcfu"
   },
   "source": [
    "## **Activation Functions**\n",
    "\n",
    "- Takes into account some kind of threshold is called an activation function\n",
    "- Mathematically, there are many such activation functions that could achieve this effect\n",
    "\n",
    "<img src=\"./images/activation.png\" width=400 align=\"center\"/>\n",
    "\n",
    "<!--\n",
    "- Non-linear activation functions are essential for neural networks to learn complex, non-linear data representations.\n",
    "- Without these activation functions, neural networks would simply be a linear function such as 𝑊_1 𝑊_2 𝑥 = 𝑊𝑥.\n",
    "  - However, by incorporating non-linear activation functions, neural networks with a large number of layers and neurons can approximate more complex functions.\n",
    "- As the number of neurons increases, the representation improves, as shown in the figure, but there is a risk of overfitting.\n",
    " -->\n",
    "\n",
    "<!-- <img src=\"./images/dl15.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO2SXiS1Ecfv"
   },
   "source": [
    "The most popular activation functions are:\n",
    "\n",
    "<img src=\"./images/activationFunctions.pbm\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBTyR9CUEcfv"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdWGb-FKEcfv"
   },
   "source": [
    "Toy Example:\n",
    "\n",
    "<img src=\"./images/dl12.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<img src=\"./images/dl13.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-SkMpfIEcfv"
   },
   "source": [
    "## **Loss Function in Deep Learning**\n",
    "\n",
    "- In deep learning, a loss function is a mathematical function that quantifies the **difference** between the **predicted** output and the **actual** output.\n",
    "  - It measures how well the neural network is performing its task\n",
    "\n",
    "- Common Loss Functions\n",
    "  -  Mean Squared Error (MSE)\n",
    "     -  The mean squared error loss function is used for regression problems, where the goal is to predict a continuous value. It calculates the average of the squared differences between the predicted and actual values.\n",
    "  -  Binary Cross-Entropy\n",
    "     -  The binary cross-entropy loss function is used for binary classification problems, where the output can take only two values (0 or 1). It measures the difference between the predicted probability and the true label.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wsko/hands-on-gen-ai-2/main/images/dl22.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPJr0XkBEcfv"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cimZmQjGEcfw"
   },
   "source": [
    "## **Goal of Training NNs**\n",
    "\n",
    "<img src=\"./images/dl23.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ymxp496Ecfw"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1WJVS4lEcfw"
   },
   "source": [
    "## **Most Popular Frameworks**\n",
    "\n",
    "Here are some of the most popular deep learning frameworks:\n",
    "\n",
    "- **Tensorflow 1.x:** This was one of the first widely available frameworks developed by Google. It allowed users to define a static computation graph, push it to the GPU, and explicitly evaluate it.\n",
    "\n",
    "- **PyTorch:** Developed by Facebook, PyTorch has been growing in popularity. It offers a flexible and dynamic approach to building neural networks.\n",
    "\n",
    "- **Keras:** Keras is a higher-level API that sits on top of both Tensorflow and PyTorch. It was created by Francois Chollet to unify and simplify the process of using neural networks.\n",
    "\n",
    "- **Tensorflow 2.x + Keras:** This is a new version of Tensorflow that integrates Keras functionality. It supports dynamic computation graphs, making tensor operations similar to those in NumPy and PyTorch.\n",
    "\n",
    "In this notebook, we will focus on using PyTorch. Make sure you have the latest version of PyTorch installed by following the [instructions on their website](https://pytorch.org/get-started/locally/). Typically, installation is as simple as running one of the following commands:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygH1USt5Ecfw"
   },
   "outputs": [],
   "source": [
    "# !%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fwtJ9-i_Ecfy",
    "outputId": "92e12123-c7b9-4aae-a8b8-56258cef0264"
   },
   "outputs": [],
   "source": [
    "'''#Or\n",
    "\n",
    "%conda install pytorch -c pytorch\n",
    "\n",
    "#This will ensure you have the necessary packages to work with PyTorch.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mHSS2k2gEcfz",
    "outputId": "b4b0c888-e35c-4e12-ed57-7277aa36c036"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajpw3i5wEcfz"
   },
   "source": [
    "## Basic Concepts: Tensor\n",
    "\n",
    "**Tensor** is a multi-dimensional array. It is very convenient to use tensors to represent different types of data:\n",
    "* 400x400 - black-and-white picture\n",
    "* 400x400x3 - color picture\n",
    "* 16x400x400x3 - minibatch of 16 color pictures\n",
    "* 25x400x400x3 - one second of 25-fps video\n",
    "* 8x25x400x400x3 - minibatch of 8 1-second videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsCVn_WsEcfz"
   },
   "source": [
    "### Simple Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cf-rAEqgEcf0",
    "outputId": "d3d39b1a-babb-440f-da0c-2f2c586bc669"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "print(a)\n",
    "a = torch.randn(size=(10,3,2))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nz1ENqMsEcf0",
    "outputId": "c1e3bb69-5a0d-4c22-f853-35555872be32"
   },
   "outputs": [],
   "source": [
    "print(a-a[0])\n",
    "print(torch.exp(a)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyiJ47EREcf1"
   },
   "source": [
    "## In-place and out-of-place Operations\n",
    "\n",
    "- Tensor operations such as `+`/`add` return new tensors.\n",
    "- Sometimes, you need to modify the existing tensor in-place.\n",
    "- Many operations have in-place counterparts, which end with `_`.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKSgeN-JEcf1",
    "outputId": "bfa6adf6-3c7c-4043-a647-782dfe0c2fd3"
   },
   "outputs": [],
   "source": [
    "u = torch.tensor(5)\n",
    "print(\"Result when adding out-of-place:\",u.add(torch.tensor(3)))\n",
    "u.add_(torch.tensor(3))\n",
    "print(\"Result after adding in-place:\", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foyVJGmQEcf1"
   },
   "source": [
    "## Computing the Sum of All Rows in a Matrix (Naive Approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBMaRL61Ecf2",
    "outputId": "7d14332d-970b-4679-e4c9-1cf3b323f168"
   },
   "outputs": [],
   "source": [
    "s = torch.zeros_like(a[0])\n",
    "for i in a:\n",
    "  s.add_(i)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwdNwH0TEcf2",
    "outputId": "9107e1ca-80a6-4ec3-f615-fb3dcfbf99cd"
   },
   "outputs": [],
   "source": [
    "#much better way:\n",
    "torch.sum(a,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YgkSW1xEcf3"
   },
   "source": [
    "See more in the [official documentation](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jIUsoA8Ecf3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9013wqUEcf3"
   },
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-rTDTGWEcf4"
   },
   "source": [
    "# Create a Simple Neural Network using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "7xyCTVXaEcf4",
    "outputId": "4e4b96fa-dfee-4b02-aa02-75cd247db4e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create a toy dataset\n",
    "# Generate some synthetic data (X and y)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(100, 1)  # Training input features\n",
    "y_train = 3 * X_train + 2 + 0.1 * np.random.randn(100, 1)  # Training labels with noise\n",
    "\n",
    "X_test = np.random.rand(20, 1)  # Test input features\n",
    "y_test = 3 * X_test + 2 + 0.1 * np.random.randn(20, 1)  # Test labels with noise\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(X_train, y_train, label='Training Data', color='blue')\n",
    "\n",
    "# Plot the test data\n",
    "plt.scatter(X_test, y_test, label='Test Data', color='red')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Scatter Plot of Toy Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zF0xaZbJEcf4"
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zibuik6oEcf5"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define the neural network model\n",
    "class SimpleLinearRegression(nn.Module):\n",
    "   def __init__(self):\n",
    "    # Constructor for the SimpleLinearRegression class.\n",
    "    # It initializes the neural network architecture.\n",
    "    super(SimpleLinearRegression, self).__init__()\n",
    "\n",
    "    # Define a linear layer with input size 1 and output size 1.\n",
    "    # This layer represents a simple linear regression model.\n",
    "    self.linear = nn.Linear(1, 1)\n",
    "\n",
    "   def forward(self, x):\n",
    "        # This function represents the forward pass of the neural network.\n",
    "        # It takes an input tensor 'x' and passes it through the linear layer.\n",
    "        # The output of this linear layer is returned as the result.\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epRd-ja-Ecf5"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = SimpleLinearRegression()\n",
    "\n",
    "# Step 3: Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVomI_NWEcf5"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qgTRbclEcf6"
   },
   "source": [
    "## **Given output error, how to update Weights?**\n",
    "\n",
    "You’re on the side of a hill and you need to get to the bottom\n",
    "\n",
    "- It’s dark and you can’t see anything.\n",
    "- You do have a torch, what do you do?\n",
    "- You don’t have an accurate map\n",
    "\n",
    "<img src=\"./images/gradient1.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>\n",
    "\n",
    "the function we’re trying to minimize is the neural network’s error\n",
    "\n",
    "\n",
    "<img src=\"./images/gradient2.png\" width=800 height=auto style=\"background-color:white;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPiFi-aNEcf6"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jR_0Poe0Ecf6",
    "outputId": "acf72f53-2f4e-4ddc-efc0-e957435d71fd"
   },
   "outputs": [],
   "source": [
    "# Step 4: Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass for training data\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGj3pGNfEcf7",
    "outputId": "e260cc4d-4c4d-4934-b2dc-d4ad84c10050"
   },
   "outputs": [],
   "source": [
    "# Step 5: Make predictions on the test set and calculate MSE\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "    print(f'Test MSE: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "kbhPb0VqEcf7",
    "outputId": "29f21f61-dee6-4e81-8c59-b6c5ba4ffcf6"
   },
   "outputs": [],
   "source": [
    "# Plot the raw data and model fit\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot raw data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, label='Raw Data', color='blue')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Raw Data')\n",
    "plt.legend()\n",
    "\n",
    "# Plot model fit\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train, y_train, label='Raw Data', color='blue')\n",
    "plt.plot(X_test, test_outputs.numpy(), label='Model Fit', color='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Model Fit')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Phr654Ecf7"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJTdkjvhEcf8"
   },
   "source": [
    "## **Excercise**\n",
    "\n",
    "Let's play with the code.\n",
    "- Try another loss function\n",
    "    <!-- huber_loss = nn.SmoothL1Loss()  # Huber loss -->\n",
    "\n",
    "- What if I want to use adam optimizer?\n",
    "<!-- optimizer = optim.Adam(model.parameters(), lr=0.001)  # Use Adam optimizer with a smaller learning rate -->\n",
    "\n",
    "- How the number of epochs might change the performance?\n",
    "  <!-- 200? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2zdyzohEcf8"
   },
   "source": [
    "<img src=\"https://github.com/wsko/Generative_AI/blob/main/Day-1/images/border.jpg?raw=1\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo3qgnWvEcf8"
   },
   "source": [
    "## **Performing Computations on GPU with PyTorch**\n",
    "\n",
    "Accelerating deep learning computations is often achieved by utilizing powerful Graphics Processing Units (GPUs). PyTorch simplifies GPU computing with straightforward steps.\n",
    "\n",
    "1. Define the target device for computations at the beginning of your code. Choose either `\"cpu\"` for CPU or `\"cuda\"` for GPU. This sets the device for tensor operations.\n",
    "\n",
    "2. Move tensors to the specified device using the `.to(device)` method. This can be done for existing tensors or during tensor creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PoOMCvuEcf9",
    "outputId": "7ffedba5-e43d-413c-fd5a-968d9c71cde2"
   },
   "outputs": [],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors and move them to the GPU\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Step 2: Define a neural network model and move it to the GPU\n",
    "class GPUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPUModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # Input size: 1, Output size: 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance of the model and move it to the GPU\n",
    "model = GPUModel().to(device)\n",
    "\n",
    "# Step 3: Use a different optimizer with adaptive learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Use Adam optimizer with a smaller learning rate\n",
    "\n",
    "# Step 4: Train the model with more epochs\n",
    "num_epochs = 2000  # Increase the number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass for training data\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 5: Calculate the Huber loss on the test set\n",
    "huber_loss = nn.SmoothL1Loss()  # Huber loss\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = huber_loss(test_outputs, y_test_tensor)\n",
    "    print(f'Test Huber Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BZtw_8QEcf9"
   },
   "source": [
    "# Train a Simple Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "je6xG8kIEcf9",
    "outputId": "1ffa93fc-17ec-4ef4-dd14-78a0cd248f51"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Step 1: Create a toy dataset\n",
    "# Generate synthetic data for binary classification\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # Input features (100 samples with 2 features)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary labels based on a simple rule\n",
    "\n",
    "# Plot the synthetic data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label='Class 0', color='blue')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label='Class 1', color='red')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Data for Binary Classification')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)  # Use LongTensor for classification labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kShbildyEcf-"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define a Feedforward Neural Network model\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the Feedforward Neural Network model\n",
    "input_size = 2  # Number of input features\n",
    "hidden_size = 4  # Number of neurons in the hidden layer\n",
    "num_classes = 2  # Number of output classes for binary classification\n",
    "model = FeedForwardNN(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj7pfsFtEcf-"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-Entropy loss for classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "pEyUr_VIEcf-",
    "outputId": "ae4cf642-b2d2-4d46-8996-84384501a066"
   },
   "outputs": [],
   "source": [
    "# Step 4: Training loop\n",
    "num_epochs = 1000\n",
    "losses = []  # Store loss values for plotting\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    losses.append(loss.item())  # Store loss for plotting\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OBrUSCbEcf_"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data for evaluation\n",
    "np.random.seed(42)\n",
    "X_test = np.random.rand(50, 2)  # Input features (100 samples with 2 features)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 1).astype(int)  # Binary labels based on a simple rule\n",
    "# Convert data to PyTorch tensors\n",
    "X_testtensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_testtensor = torch.tensor(y_test, dtype=torch.long)  # Use LongTensor for classification labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "rdhbEy7YEcf_",
    "outputId": "2183fd0c-de2e-4721-c237-6efc7bc8de2b"
   },
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the model\n",
    "with torch.no_grad():\n",
    "    # Generate predictions for the entire dataset\n",
    "    all_predictions = model(X_testtensor)\n",
    "    predicted_classes = torch.argmax(all_predictions, dim=1).numpy()\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, predicted_classes)\n",
    "    precision = precision_score(y_test, predicted_classes)\n",
    "    recall = recall_score(y_test, predicted_classes)\n",
    "    f1 = f1_score(y_test, predicted_classes)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB7kI1nNNZt4"
   },
   "source": [
    "# Train a Simple Feedforward Neural Network - using Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSrg68DSMNuA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define a Feedforward Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='relu'))  # Input layer to hidden layer\n",
    "model.add(Dense(2, activation='softmax'))  # Hidden layer to output layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "xAJuxfoGNgih",
    "outputId": "7b7a1153-4c03-4109-d16f-68a2990ef682"
   },
   "outputs": [],
   "source": [
    "# Step 3: Compile the model\n",
    "model.compile(optimizer=SGD(learning_rate=0.1),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Training the model\n",
    "num_epochs = 1000\n",
    "history = model.fit(X, y, epochs=num_epochs, verbose=0)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "dQ55msK8NoX9",
    "outputId": "260dbae9-7459-46d0-db3c-713d58ec6492"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Generate synthetic data for evaluation\n",
    "np.random.seed(42)\n",
    "X_test = np.random.rand(50, 2)  # Input features (50 samples with 2 features)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 1).astype(int)  # Binary labels based on a simple rule\n",
    "\n",
    "# Evaluate the model\n",
    "predicted_classes = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "precision = precision_score(y_test, predicted_classes)\n",
    "recall = recall_score(y_test, predicted_classes)\n",
    "f1 = f1_score(y_test, predicted_classes)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kunp-spYEcf_"
   },
   "source": [
    "# Multi-Layered Neural Network: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "id": "z_2UXWpQEcgA",
    "outputId": "db23b456-6fbc-4e0d-ddd9-dc88705b2966"
   },
   "outputs": [],
   "source": [
    "# Define a Multi-Layered Neural Network model\n",
    "class MultiLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(MultiLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # Input layer to hidden layer 1\n",
    "        self.relu1 = nn.ReLU()  # ReLU activation for hidden layer 1\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Hidden layer 1 to hidden layer 2\n",
    "        self.relu2 = nn.ReLU()  # ReLU activation for hidden layer 2\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)  # Hidden layer 2 to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the Multi-Layered Neural Network model\n",
    "input_size = 2  # Number of input features\n",
    "hidden_size1 = 8  # Number of neurons in hidden layer 1\n",
    "hidden_size2 = 4  # Number of neurons in hidden layer 2\n",
    "num_classes = 2  # Number of output classes for binary classification\n",
    "model = MultiLayerNN(input_size, hidden_size1, hidden_size2, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-Entropy loss for classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "losses = []  # Store loss values for plotting\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    losses.append(loss.item())  # Store loss for plotting\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "WVkH1NOeEcgA",
    "outputId": "98e87bc6-f29e-4725-9d2e-a1156c20ecce"
   },
   "outputs": [],
   "source": [
    "# Step Evaluate the model\n",
    "with torch.no_grad():\n",
    "    # Generate predictions for the entire dataset\n",
    "    all_predictions = model(X_tensor)\n",
    "    predicted_classes = torch.argmax(all_predictions, dim=1).numpy()\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y, predicted_classes)\n",
    "    precision = precision_score(y, predicted_classes)\n",
    "    recall = recall_score(y, predicted_classes)\n",
    "    f1 = f1_score(y, predicted_classes)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y, predicted_classes)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFnSd-EQAsyd"
   },
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "# **LAB**: hand-written image recognition\n",
    "\n",
    "\n",
    "- https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "\n",
    "<img src=\"./images/MNIST_dataset.png\"  width = \"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "w_y9Qi2PB5B0",
    "outputId": "dac0f26a-a56c-4cdd-be8d-ef8758deebdc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Visualizing 10 training images in one row with labels\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.xticks([])  # Remove x tick marks\n",
    "    plt.yticks([])  # Remove y tick marks\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Select 10 images\n",
    "images = images[:10]\n",
    "labels = labels[:10]\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images, nrow=10))\n",
    "\n",
    "# Print labels\n",
    "print(' '.join(f'{labels[j].item()}' for j in range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y_JXsr7AB-s2",
    "outputId": "0caf97cb-f100-499b-c300-1096ccef0cae"
   },
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  #reshape the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHn0nT_mFPKh",
    "outputId": "efdd6583-e5ba-43ea-dd79-46b1cfd11ace"
   },
   "outputs": [],
   "source": [
    "# Evaluating the network on the test set\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(16):  # batch size is 16\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvfGcNARjGNB"
   },
   "source": [
    "## TensorFlow solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "RwAziffEjITS",
    "outputId": "61c96f78-4f8d-4bd4-9ed0-7f55fff73e14"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Visualize 10 random images from the training set\n",
    "def plot_images(images, labels):\n",
    "    plt.figure(figsize=(10, 1))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(labels[i])\n",
    "    plt.show()\n",
    "\n",
    "# Select 10 random images\n",
    "indices = np.random.choice(range(len(x_train)), 10)\n",
    "selected_images = x_train[indices]\n",
    "selected_labels = y_train[indices]\n",
    "\n",
    "plot_images(selected_images, selected_labels)\n",
    "\n",
    "# Build the neural network\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the network\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the network\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "predictions = model.predict(x_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    label = y_test[i]\n",
    "    if predicted_labels[i] == label:\n",
    "        class_correct[label] += 1\n",
    "    class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jMMfP0IjMbk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
