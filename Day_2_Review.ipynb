{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOp/GZWHQ9xKpEYjujx9iv1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- **Neural Net**: A computational model inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers to process input data.\n","\n","- **Neuron (Node or, simply, Unit)**: A basic unit in a neural network that receives input, applies a weight, sums the inputs, and passes the result through an activation function.\n","\n","- **Activation Function**: A function applied to a neuron's output to introduce non-linearity, allowing the network to learn complex patterns (e.g., ReLU, sigmoid, tanh).\n","\n","- **Weight**: A parameter in a neural network that multiplies the input signal; it is adjusted during training to minimize the error in predictions.\n","\n","- **Forward Propagation**: The process of passing input data through the network layers to obtain an output prediction.\n","\n","- **Backpropagation**: The algorithm used to update the weights in a neural network by calculating the gradient of the loss function with respect to each weight, propagating the error backward through the network.\n","\n","- **Gradient Descent**: An optimization algorithm used to minimize the loss function by iteratively adjusting the weights in the direction of the steepest descent.\n","\n","- **Optimizer**: A method or algorithm (e.g., SGD, Adam) used to adjust the weights in the network based on the gradients calculated during backpropagation by dynammically adjusting the learning rate.\n","\n","- **Batch**: A subset of the training data used to train the network in one iteration, improving training efficiency and stability.\n","\n","- **Learning Rate**: A hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\n","\n","- **Loss Function**: A function that measures the difference between the predicted output and the actual target, guiding the optimization process.\n","\n","- **Epoch**: One complete pass through the entire training dataset during the training process.\n","\n","- **Hyperparameter**: A parameter set before the learning process begins, which controls the learning process (e.g., learning rate, batch size).\n","\n","- **Overfit**: A situation where a model learns the training data too well, including its noise and outliers, resulting in poor generalization to new, unseen data.\n","\n","- **Regularization**: Techniques (e.g., L1, L2 regularization, dropout) used to prevent overfitting by adding constraints or noise to the model.\n","\n","- **Dropout**: A regularization method where random neurons are ignored during training, reducing the risk of overfitting and improving generalization."],"metadata":{"id":"7ZZB29w2kLh8"}},{"cell_type":"code","source":[],"metadata":{"id":"Tnkc3gIwkMXT"},"execution_count":null,"outputs":[]}]}